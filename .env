# .env

# API_KEY
OPENAI_API_KEY=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
ANTHROPIC_API_KEY=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

# Work_Dir_Path
LOG_DIR=/mnt/e/CoT_Result/logs
BACKUP_DIR=/mnt/e/CoT_Result/backups
DATASET_PATH=
RESULTS_DIR=

# LLM generation parameters
TEMPERATURE=0.3
MAX_NEW_TOKENS=1024
TOP_P=0.9
TOP_K=50
REPETITION_PENALTY=1.2

# Reward function hyperparameters
# θc: complexity threshold, θa: answer correctness threshold
THETA_C=0.02  # θc: complexity alignment tolerance
THETA_A=0.02  # θa: correctness alignment tolerance
# Reward weights (How vs Which)
REWARD_COMPLEXITY=0.65  # weight for process complexity (How)
REWARD_CORRECTNESS=1 - REWARD_ALPHA  # weight for answer correctness (Which)
BERTSCORE_THRESHOLD=0.92
REWARD_WINDOW_SIZE=2

# The count of sentences for one item cannot be more than 3x the count for another.
WHW_RULES_MIN_TOTAL_SENTENCE=6
WHW_RULES_MAX_BALANCE_RATIO=2.99

# Experiment Settings
NUM_QUESTIONS=10
START_TRIAL=4
TOTAL_RUNS=16
DEFAULT_SEED=42