# .env Template

# API_KEY
OPENAI_API_KEY =
ANTHROPIC_API_KEY =

# Work_Dir_Path
LOG_DIR =
BACKUP_DIR =
DATASET_PATH =
RESULTS_DIR =

# LLM generation parameters
TEMPERATURE = 0.3
MAX_NEW_TOKENS = 1024
TOP_P = 0.9
TOP_K = 50
REPETITION_PENALTY = 1.2

# Reward function hyperparameters
# θc: complexity threshold, θa: answer correctness threshold
THETA_C = 0.02  # θc: complexity alignment tolerance
THETA_A = 0.02  # θa: correctness alignment tolerance
# Reward weights (How vs Which)
REWARD_COMPLEXITY = 0.65  # weight for process complexity (How)
REWARD_CORRECTNESS = 1 - REWARD_ALPHA  # weight for answer correctness (Which)
BERTSCORE_THRESHOLD = 0.92
REWARD_WINDOW_SIZE = 2

# The count of sentences for one item cannot be more than 3x the count for another.
WHW_RULES_MIN_TOTAL_SENTENCE = 6
WHW_RULES_MAX_BALANCE_RATIO = 2.99

# Experiment Settings
TOTAL_RUNS = 16
DEFAULT_SEED = 42